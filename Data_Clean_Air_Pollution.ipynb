{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retreval and Cleanup Process - Air Pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents<br>\n",
    "* [Retrieving and Clean Data - AQS.EPA.GOV](#aqs)\n",
    "* [Census (Ppoulation)](#nextset)\n",
    "* [Next set](#nextset)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving and Clean Data - AQS.EPA.GOV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Air pollution data will be filtered to only retrieve information we will use. The json will be saved to a .CSV and analyzed in other notebooks.<br><br>\n",
    "A function was created with the objective to acquire the dataset by passing specific parameters required for the API. The API request specific state and county numbers, as well as the api key, begin date, end date, and param code. The function requires a state code (ie: '06'), county code (ie: '037'), and a state ('California'). All paramters in string formatting.\n",
    "\n",
    "A tuple, '[(\"06\", \"037\", \"California\")]', is passed under the variable state_countiesinto the function 'StateCountySampleDataDF(state_counties, params_air, dict_state_county_df)'. The function returns a dictionary of DataFrames. Each DataFrame is named by the tuple passed, 'dict_state_county_df['df_California_06_037']'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./annual_seasonal_notebooks/images/dataclean.png\" alt=\"laair\"\n",
    "title=\"LA Air\" width=\"500\" height=\"300\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dict is created and verified for content, it is saved as a .csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./annual_seasonal_notebooks/images/save_to_dict.png\" alt=\"laair\"\n",
    "title=\"LA Air\" width=\"500\" height=\"300\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census website have multiple csv file that shows population per city/county on different year.\n",
    "\n",
    "csv files were downloaded then called using pd.read() method.\n",
    "\n",
    "Once called, unecessary years were dropped from the dataframe to clean the data frame.\n",
    "\n",
    "With the city name not exactly matching between the EPA dataframe city name and census dataframe, fuzzymatching was utilized to rename the city name in EPA dataframe. After fuzzymatching, columns not being used were dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPA Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Population_Location _notebooks/plot/EPA_Dataframe.png\" alt=\"laair\"\n",
    "  width=\"800\" height=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Population_Location _notebooks/plot/Census_dataframe.png\" alt=\"laair\"\n",
    "  width=\"300\" height=\"100\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzymatch Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Population_Location _notebooks/plot/fuzzymatch.png\" alt=\"laair\"\n",
    "  width=\"500\" height=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Due to having a standard account, there was a limit of how many tweets that one could pull, so I pulled a 100 tweets per hashtag.\n",
    "* Was able to find 25 trending hashtags that were relatable on air pollution from Allhashtag.com.\n",
    "* Data had to be pulled twice due to being timed out from twitter. tqdm was used in order to speed up this process.\n",
    "* A location list and text list were created inorder to store the data.\n",
    "* Could not pull tweets based on geo coordinates since my account was standard, location parameter was used instead.\n",
    "* The tweets pulled were global, but the information is for the US only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Twitter_notebooks/images/hashtags.png\" alt=\"nyair\"\n",
    "title=\"nyair\" width=\"700\" height=\"500\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Created state codes in the location parameter in order to clean the global data and show only state information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Twitter_notebooks/images/state_codes.png\" alt=\"nyair\"\n",
    "title=\"nyair\" width=\"500\" height=\"300\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Used Stopwords in order to clean the text and added some words that I wanted to remove in order to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Twitter_notebooks/images/cleantext.png\" alt=\"nyair\"\n",
    "title=\"nyair\" width=\"700\" height=\"500\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
